---
title: "mahalanobis"
---

%% The point to this section is to show the distance is a measurement of similarity. The more similar the two vectors to each other the smaller of the distance. Then we indicate the probability function of chi-square distribution, to provide a new selection criterion with p-values. %%

Euclidean distance is a commonly used measure of similarity between two vectors in the space. Assume we have two $n$-length vectors $X = (x_1, x_2, ... x_n)$ and $Y = (y_1, y_2, ..., y_n)$, the Euclidean distance can be expressed as:

$$
D_{Eld} (X, Y) = \|X - Y \| = \sqrt{\sum_{i = 1}^{n} (x_i - y_i)^2}
$$

Euclidean distance similarity measure (EDSM) indicates that the smaller of the value of Euclidean distance $D_{Eld}$, the more similar between vector $X$ and $Y$. However, one of the limitation of Euclidean distance is the assumption of independence of samples. In practice, this assumption cannot be fulfilled in reality. For our application, the children growth at give time may correlated and share similar trend. Covariance matrices are used to characterize the cross correlation between noises at different interesting points. It is possible to account for the cross-correlation between the samples and obtain more accurate distance measures with covariance matrices. Hence, we exploit the similarity measurement with Mahalanobis distance similarity measure (MDSM).

Developed by Prasanta Chandra Mahalanobis in 1936, Mahalanobis distance is a powerful tool for measuring the difference between an individual's profile and the population average. For example, it can be used to assess how the children with a particular disorder or injury is different from the general population or a specific subpopulation. With the same two $n$-length vectors $X$ and $Y$, the Mahalanobis distance can be expressed as:

$$
D_{Mhl} = \sqrt{(X - Y)^{\top} \Sigma ^{-1} (X - Y)}
$$

where $(X - Y) = (x_1 - y_1, x_2 - y_2, ..., x_n - y_n)$ and $\Sigma$ is population covariance matrix.

MDSM has been widely used, as a state-of-the-art method, to detect out-of-distribution and adversarial examples (need citations). In contrast to Euclidean distances, Mahalanobis distance is a data-driven measure that are independent of the related dataset to which two data points belong. The Mahalanobis distance similarity measure is most commonly used to measure the similarity of two vectors while taking into account the correlation between different variables.

From a different perspective, Mahalanobis metric can be viewed as linearly transformed Euclidean distance metric. We can regard Mahalanobis distance as the distances between variables are scaled according to the covariance structure of the data. In such way, Mahalanobis distance can quantify the similarity of samples and to better measure the similarity of vectors compared to Euclidean distances.

When the population follows a multivariate normal distribution (MVN), the squared Mahalanobis distance, $D_{M}^2 = {(X - Y)^{\top} \Sigma ^{-1} (X - Y)}$, follows a central chi-square distribution with $df = n$. (The commonly used estimates of probability $P$ for Mahalanobis distances are the $p$-value computed from the $\chi^2$ distribution of $D_M^2$ or the $p$-value from the central $F$-distribution associated with Hotelling's $t^2$ test. $\chi^2$ is used for computational convinience.)

In this paper, we focus on estimating a probability measure $P$, as the proportion of the population that gives a more unusual Mahalanobis comparing to target $X^*$, under MVN distribution assumption. The larger of the Mahalanobis distance, the less similar between $X$ and target $X^*$. Hence we have the probability $P$ and the acceptance region $A$ for matching information donor as:

$$
P(X , X^* similar)\\
P_X = Pr \Big\{ {(X - X^*)^{\top} \Sigma ^{-1} (X - X^*)}) \leq \chi^2_{df = n,\ \alpha}\Big\} \\
A = \{X: P_X \leq \alpha \}
$$

Hence, the critical probability value $\alpha$ can be used as a flexible selection threshold to decide the set and size of matching donor cohort. The larger of the $\alpha$ value, the more restrict on the similarity selection of the donors.

find a good point estimator of the abnormality of a Mahalanobis index. The answer is less straightforward, as the best choice of estimator will depend on the purpose for which the estimator is required.
